# Percussive Generator

Using miniconda to manage env

- create envirioment from `envirioment.yml`

        conda env create -f environment.yml

- activate

        conda activate perc-gen

Linux only.  Deps will most likely not install on OSX.

Quick breakdown of this repo. 

### `data_from_airtable.py`

This script downloads audio data from the BetterProblems s3 bucket. For now, this is the dataset being used to train the model.

- The script expects a csv file download from airtable saved as `./data/temp-data.csv`
- this csv **MUST** contain a `Recording_S3_LocationLink` column. 
- You may set `NB_DOWNLOADS` to specify how many recordings you whish to download. Default is `1000`.
- The script will output a csv at `data/recordings.csv` that will be later used by `split_audio.py` scripts.
- audio files are saved in `./data/audio`

### `split_audio.py`

This script will split the downloaded audio into chunks. You must provide the csv path generated by
by `data_from_airtable.py`.

        $ python split_audio data/recordings.csv

- By default, chunks are of 10 seconds. This can be adjusted by setting the constant `MAX_LEN`.
- This script expects a `temp` column in the input CSV. For testing, we are using the temperature as our label. This should be adjusted as we progress in this research.
- As new csv files `data/split_audio.csv` will be saved with the chunks and their labels (temperature or `temp` for now).
- Split audio files are saved in `data/split_audio`

### `audio_dataset.py`

A custom pytorch dataset class that loads the audio files and labels. You can provide the path to the csv file when instantiating

    from audio_dataset import AudioDataset
    audio_dataset = AudioDataset('path/to/csv/file.csv', device='cpu')

You may also provide a transform chain, sample rate as well as the length of the audio samples. 

    audio_dataset = AudioDataset('data/split_audio.csv', transform=transforms, audio_len_seconds=20, sample_rate=16000, device='cuda')

- note that `AudioDataset` will trim or pad any audio file that is not equals to `audio_len_seconds`
- This class expects a column `temp` as being the label to predict and `audio_path` as the path to the audio files to load.
- Default device is `cpu` but you can provide `cuda` as argument.

### `perc_gen.py`

This is where the model and training loop lives. This model uses a `wav2vec2` behind the curtains.

- Instantiating `Wav2Vec2Regression` will download the base `wav2vec2` from the hugging face transformers library.
- Running the training loop will download the `wav2vec2` processor 
- `train` will return a list with the average epoch loss.
- By default, the pre-trained parameters are frozen, but you can pass a `freeze=false` to fine-tune on all model parameters.

example

    from perc_gen import Wav2Vec2Regression, train

    w2v2_regression= Wav2Vec2Regression(freeze=True)

    audio_dataset = AudioDataset('data/split_audio.csv')
    train_loader = DataLoader(dataset=audio_dataset, shuffle=True, batch_size=10)

    criterion = torch.nn.MSELoss()
    optim = optim.AdamW(params=w2v2_regression.parameters(), lr=0.001)

    train(w2v2_regression,
          dataloader=train_loader,
          criterion=criterion,
          optim=optim,
          sample_rate=audio_dataset.sample_rate,
         )


see `how_to.ipynb` to see the code above in action.



